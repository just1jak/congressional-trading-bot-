# AI-in-the-Loop Optimization System - Guide

## Overview

The Congressional Trading Bot now includes a sophisticated AI optimization layer that continuously learns from trading performance and automatically improves bot parameters. This guide covers Phase 1 (Foundation) implementation.

## What's New in Phase 1

### Database Tables
Six new tables track optimization data:
- `optimization_metrics` - Performance metrics over time (Sharpe, win rate, drawdown, etc.)
- `signal_accuracy` - Signal predictions vs actual outcomes
- `parameter_history` - All parameter changes with before/after performance
- `approval_requests` - Queue for major changes requiring human approval
- `ml_model_versions` - ML model tracking and versioning
- `optimization_insights` - AI-generated insights and learnings

### Real-Time Metrics Collection

The system now automatically tracks:
- **Signal accuracy** by conflict resolution method (dollar_weighted, unanimous, track_record)
- **Trade outcomes** (profit/loss, duration, exit reasons)
- **Rolling performance** windows (1d, 7d, 30d, 90d)
- **Risk metrics** (Sharpe ratio, max drawdown, win rate, profit factor)

### Multi-Objective Scoring

Performance is evaluated using a weighted composite score:
```
score = 0.30 × returns + 0.25 × sharpe + 0.20 × win_rate
        + 0.15 × (1 - drawdown) + 0.10 × profit_factor
```

### CLI Commands

New `optimize` command group:

```bash
# View optimization status and metrics
python -m src.cli.cli optimize status --window 30

# Manually collect and calculate metrics
python -m src.cli.cli optimize collect-metrics --window 30

# Review pending approval requests
python -m src.cli.cli optimize review-pending

# View AI-generated insights
python -m src.cli.cli optimize insights --days 30
```

## Installation

### 1. Install New Dependencies

```bash
pip install -r requirements.txt
```

This installs:
- `xgboost` - ML model for signal confidence prediction
- `scikit-learn` - ML utilities and Random Forest
- `tensorflow` - LSTM for exit timing optimization (Phase 5)
- `optuna` - Bayesian hyperparameter optimization
- `anthropic` - Claude API for LLM analysis (Phase 4)
- `schedule` - Background task scheduling

### 2. Update Database Schema

The new tables will be created automatically on next run:

```bash
python -m src.cli.cli status
```

Or manually initialize:

```python
from src.data.database import init_database
db = init_database()
db.create_tables()  # Creates all tables including new optimization tables
```

### 3. Configuration

The optimization system uses `config/optimization_config.yaml`. Default settings are production-ready, but you can customize:

**Key settings:**
- `optimization.enabled` - Master on/off switch
- `objectives.*_weight` - Multi-objective score weights
- `parameter_bounds.*` - Safety limits for parameter changes
- `llm.api_key_env_var` - Environment variable for Claude API key (Phase 4)

## Usage

### Automatic Metrics Collection

Metrics are automatically collected whenever signals are generated. The system tracks:

1. **Signal Recording** - Every signal generated by `SignalGenerator` is recorded
2. **Outcome Tracking** - When trades close, outcomes are linked to original signals
3. **Performance Calculation** - Metrics are computed in rolling windows

### Manual Metrics Collection

Force a metrics calculation:

```bash
python -m src.cli.cli optimize collect-metrics --window 30
```

This calculates and stores:
- Win rate
- Average return
- Sharpe ratio
- Max drawdown
- Profit factor
- Total trades

### Viewing Performance

Check optimization status:

```bash
python -m src.cli.cli optimize status --window 30
```

Output includes:
- **Composite Score** - Multi-objective performance score (0-1)
- **Component Scores** - Breakdown by objective (returns, Sharpe, etc.)
- **Raw Metrics** - Actual performance numbers
- **Signal Accuracy** - Accuracy by conflict resolution method
- **Degradation Warnings** - Alerts if performance declining

### Understanding Composite Score

The composite score ranges from 0.0 (worst) to 1.0 (best):

- **0.7-1.0** - Excellent performance (green)
- **0.5-0.7** - Good performance (yellow)
- **0.0-0.5** - Poor performance (red)

Components are normalized and weighted:
- Returns: -20% to +20% → 0 to 1
- Sharpe: -2 to +3 → 0 to 1
- Win Rate: Direct (already 0-1)
- Drawdown: Inverted (lower is better)
- Profit Factor: 0 to 3 → 0 to 1

### Signal Accuracy Analysis

The system tracks which conflict resolution method performs best:

```bash
python -m src.cli.cli optimize status --window 30
```

Look for the "Signal Accuracy by Strategy" table:
- **dollar_weighted** - Dollar amount weighting
- **unanimous_only** - Only unanimous signals
- **senator_track_record** - Track record weighting (Phase 2+)

Metrics shown:
- Accuracy (% of profitable signals)
- Total signals generated
- Average P&L per signal

## Integration Points

### Signal Generator

`src/strategy/signal_generator.py` automatically records signals:

```python
# This happens automatically in analyze_ticker()
collector.record_signal(
    ticker=ticker,
    signal=signal.signal.value,
    confidence=signal.confidence,
    conflict_resolution_method=self.conflict_resolution
)
```

### Trade Execution

To track outcomes, add to your trade execution code:

```python
from src.optimization.metrics_collector import MetricsCollector

collector = MetricsCollector()

# When closing a trade
collector.record_trade_outcome(
    ticker=ticker,
    pnl_pct=profit_loss_pct,
    executed_trade_id=trade.id
)
```

## Architecture

```
User Request
    ↓
SignalGenerator.analyze_ticker()
    ↓
[Signal Generated]
    ↓
MetricsCollector.record_signal() ← Automatic tracking
    ↓
[Trade Executed]
    ↓
[Trade Closed]
    ↓
MetricsCollector.record_trade_outcome() ← Manual call needed
    ↓
MetricsCollector.calculate_and_store_metrics() ← Periodic/manual
    ↓
PerformanceAnalyzer.calculate_composite_score()
    ↓
[Optimization decisions - Phase 3+]
```

## Configuration Reference

### Multi-Objective Weights

Edit `config/optimization_config.yaml`:

```yaml
objectives:
  returns_weight: 0.30        # Maximize returns
  sharpe_ratio_weight: 0.25   # Risk-adjusted returns
  win_rate_weight: 0.20       # Consistency
  drawdown_weight: 0.15       # Capital preservation
  profit_factor_weight: 0.10  # Profit efficiency
```

**Important:** Weights must sum to 1.0

### Parameter Bounds

Safety limits prevent unsafe changes:

```yaml
parameter_bounds:
  profit_threshold:
    min: 0.15
    max: 0.30
    default: 0.20
  stop_loss:
    min: -0.20
    max: -0.05
    default: -0.10
```

### Performance Degradation

Triggers rollback if performance drops:

```yaml
optimization:
  performance_degradation_threshold: -0.10  # 10% drop
  rollback_lookback_days: 7
```

## Troubleshooting

### No Metrics Available

**Symptom:** `optimize status` shows "No optimization data available"

**Solution:**
1. Ensure trades have been executed and closed
2. Manually trigger collection: `optimize collect-metrics`
3. Check that `optimization.enabled: true` in config

### Import Errors

**Symptom:** `ImportError: No module named 'xgboost'`

**Solution:**
```bash
pip install -r requirements.txt
```

### Database Errors

**Symptom:** Table doesn't exist errors

**Solution:**
```python
from src.data.database import init_database
db = init_database()
db.drop_all_tables()  # WARNING: Deletes all data
db.create_tables()
```

## Testing

Run optimization tests:

```bash
pytest tests/test_optimization/
```

Test coverage includes:
- Signal recording and outcome tracking
- Metrics calculation
- Performance analysis
- Normalization functions
- Edge cases (no data, empty metrics)

## What's Coming Next

### Phase 2: ML Foundation (Weeks 3-4)
- XGBoost confidence predictor (replaces simple confidence calculation)
- Automated backtesting orchestrator
- Model versioning and deployment
- Parameter sweep optimization

### Phase 3: Safe Parameter Adjustment (Weeks 5-6)
- Automatic minor parameter adjustments
- Approval queue for major changes
- Rollback mechanisms
- Performance-based optimization

### Phase 4: LLM Integration (Week 7)
- Claude API integration
- Weekly performance reports
- Trade post-mortems
- Knowledge base for insights

### Phase 5: Advanced ML (Weeks 8-9)
- LSTM exit timing optimizer
- Random Forest strategy selector
- Bayesian optimization (Optuna)
- Market regime detection

### Phase 6: Continuous Service (Week 10)
- Background daemon
- Real-time optimization loop
- Production deployment
- Full automation

## API Reference

### MetricsCollector

```python
from src.optimization.metrics_collector import MetricsCollector

collector = MetricsCollector()

# Record a signal
collector.record_signal(
    ticker='AAPL',
    signal='BUY',
    confidence=0.85,
    conflict_resolution_method='dollar_weighted'
)

# Record outcome
collector.record_trade_outcome(
    ticker='AAPL',
    pnl_pct=0.12,
    executed_trade_id=123
)

# Calculate metrics
metrics = collector.calculate_and_store_metrics(window_days=30)

# Get recent metrics
recent = collector.get_recent_metrics(window_days=30, lookback_hours=24)

# Get accuracy by method
accuracy = collector.get_signal_accuracy_by_method(days_back=30)
```

### PerformanceAnalyzer

```python
from src.optimization.performance_analyzer import PerformanceAnalyzer

analyzer = PerformanceAnalyzer()

# Calculate composite score
score, components = analyzer.calculate_composite_score(window_days=30)

# Detect degradation
is_degraded, reason = analyzer.detect_performance_degradation(
    window_days=30,
    threshold=-0.10
)

# Get full summary
summary = analyzer.get_performance_summary(window_days=30)

# Compare strategies
strategy_scores = analyzer.compare_strategies(window_days=30)
```

## Best Practices

1. **Regular Monitoring** - Check `optimize status` weekly
2. **Let Data Accumulate** - Need 20+ closed trades for meaningful metrics
3. **Adjust Weights** - Tune objective weights based on your priorities
4. **Review Accuracy** - Monitor which conflict resolution method works best
5. **Test First** - Use paper trading to validate optimization behavior

## Support

For issues or questions:
1. Check logs: `logs/optimization.log`
2. Run tests: `pytest tests/test_optimization/`
3. Review this guide
4. Check GitHub issues (when project is published)

## Contributing

The optimization system is modular and extensible. To add new features:

1. **New Metrics** - Add to `MetricsCollector._calculate_metrics()`
2. **New Objectives** - Add to `PerformanceAnalyzer` with normalization
3. **New Models** - Add to `src/optimization/models/`
4. **New Insights** - Store in `OptimizationInsight` table

## License

Same as main project (see LICENSE file)
